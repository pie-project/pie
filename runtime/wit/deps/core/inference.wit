interface inference {

    use wasi:io/poll@0.2.4.{pollable};
    use types.{error};
    use context.{context};
    use model.{model, tokenizer};
    use adapter.{adapter};
    use context.{page-id};

    // binary run-length encoding
    type brle = list<u32>;

    resource forward-pass {
        constructor(model: borrow<model>);

        context: func(
            context: borrow<context>,
        );

        input-tokens: func(
            tokens: list<u32>,
            positions: list<u32>,
        );

        input-speculative-tokens: func(
            tokens: list<u32>,
            positions: list<u32>,
        );

        // enabled by default
        output-speculative-tokens: func(flag: bool);

        // if not provided, fallback to causal mask
        attention-mask: func(
            mask: list<brle>,
        );

        // if not provided, fallback to all ones (no masking)
        logit-mask: func(
            mask: brle,
        );

        sampler: func(
            indices: list<u32>,
            sampler: sampler,
        );

        adapter: func(
            adapter: borrow<adapter>,
        );

        execute: func() -> result<future-output, error>;
    }

    variant sampler {
        multinomial(tuple<f32, u32>),
        top-k(tuple<f32, u32>),
        top-p(tuple<f32, f32>),
        min-p(tuple<f32, f32>),
        top-k-top-p(tuple<f32, u32, f32>),
        embedding,
        dist(tuple<f32, u32>),
    }

    resource future-output {
        // Returns a pollable object to check when the result is ready
        pollable: func() -> pollable;

        get: func() -> option<output>;
    }

    variant output {
        none,
        tokens(list<u32>),
        tokens-with-speculation(tuple<list<u32>, list<u32>, list<u32>>), // accepted tokens, next spec tokens, next spec positions
        embeddings(list<list<u8>>),
        distributions(list<tuple<list<u32>, list<f32>>>) // Each tuple: (token IDs, associated probabilities)
    }

    /// Describes the structure that the LLM output must conform to.
    /// Constructed via static factory methods for each supported format.
    resource grammar {
        /// Construct from a JSON Schema string.
        from-json-schema: static func(schema: string) -> result<grammar, error>;

        /// Construct a built-in free-form JSON grammar (any valid JSON).
        json: static func() -> grammar;

        /// Construct from a regular expression pattern.
        from-regex: static func(pattern: string) -> result<grammar, error>;

        /// Construct from an EBNF grammar string.
        from-ebnf: static func(ebnf: string) -> result<grammar, error>;
    }

    /// Stateful matcher that walks the grammar automaton as tokens are
    /// accepted, producing a bitmask of valid next tokens at each step.
    /// The host compiles the grammar on construction and may cache the
    /// compiled result internally.
    resource matcher {
        /// Create a new matcher from a grammar and tokenizer.
        constructor(grammar: borrow<grammar>, tokenizer: borrow<tokenizer>);

        /// Accept one or more decoded tokens, advancing the matcher state.
        /// Returns an error if any token violates the grammar.
        accept-tokens: func(token-ids: list<u32>) -> result<_, error>;

        /// Fill the next-token bitmask.  The returned BRLE encodes which
        /// token ids in the vocabulary are allowed at the current position.
        next-token-logit-mask: func() -> brle;

        /// Check whether the matcher has reached a terminal state.
        is-terminated: func() -> bool;

        /// Reset the matcher to its initial state so it can be reused.
        reset: func();
    }

}