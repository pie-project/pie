interface model {

    use types.{error};

    // How the template handles system messages
    enum system-handling {
        // System is a separate turn (Qwen3, Llama3, OLMo3)
        standalone,
        // System content merged into first user turn (Gemma2, Mistral3)
        merge-with-user,
        // System placed before all turns, unwrapped (R1)
        bare-prepend,
    }

    // Chat template configuration for prompt formatting.
    // Replaces Jinja templates with a flat data structure.
    // Each role's turn is rendered as: role_prefix + content + role_suffix.
    record chat-template {

        // Token prepended at the start of the sequence
        start-token: string,

        // Tokens that end generation
        stop-tokens: list<string>,

        // Per-role wrapping: list of (role, prefix) pairs
        role-prefixes: list<tuple<string, string>>,

        // Per-role wrapping: list of (role, suffix) pairs
        role-suffixes: list<tuple<string, string>>,

        // How system messages are handled
        system-handling: system-handling,

        // Separator between system and user content when merging
        system-separator: string,

        // Header appended to start the model's response turn
        generation-header: string,

        // Wraps reasoning content before main content
        thinking-prefix: string,
        thinking-suffix: string,

        // Tool call formatting: format string with {name}, {arguments}
        tool-call-template: string,
        tool-calls-prefix: string,
        tool-calls-suffix: string,

        // Tool response formatting
        tool-response-role: string,
        tool-response-prefix: string,
        tool-response-suffix: string,
    }

    // Resource representing a specific model instance
    resource model {
        load: static func(name: string) -> result<model, error>;
        chat-template: func() -> chat-template;
        tokenizer: func() -> tokenizer;
    }

    // Tokenizer resource for encoding and decoding text
    resource tokenizer {

        // Converts input text into a list of token IDs
        encode: func(text: string) -> list<u32>;

        // Converts token IDs back into a decoded string
        decode: func(tokens: list<u32>) -> result<string, error>;

        // Returns the tokenizer's vocabulary as a list of byte sequences (tokens)
        vocabs: func() -> tuple<list<u32>, list<list<u8>>>;

        // Returns the split regular expression used by the tokenizer
        split-regex: func() -> string;

        // Returns the special tokens recognized by the tokenizer
        special-tokens: func() -> tuple<list<u32>, list<list<u8>>>;
    }
}