---
sidebar_position: 0
title: Welcome
---

# Welcome to Pie

:::caution Research Prototype
Pie is currently a **research prototype** undergoing rapid development. It is pre-release software intended for **testing and research purposes only**. APIs and features may change without notice.
:::

Pie is a programmable system for LLM serving. It combines a high-performance inference engine with a WebAssembly-based runtime that lets you run custom logic—called **Inferlets**—directly alongside your models.

To learn more about Pie's architecture and why it exists, see the [Overview](./overview) page.

## Getting Started

Ready to dive in? Head to the [Getting Started](./getting-started) guide to install Pie, then follow the [Quickstart](./getting-started/quickstart) to run your first model.

## Writing Inferlets

Want to build custom inference logic? The [Writing Inferlets](./writing-inferlets) guide covers everything from scaffolding a project to advanced techniques like speculative decoding, structured generation, and tool calling.

## Learn More

- [Overview](./overview) — Understand Pie's architecture and core concepts
- [Getting Started](./getting-started) — Install and set up Pie
- [Writing Inferlets](./writing-inferlets) — Build custom inference programs
- [Client API](./client-api/python) — Programmatic access to Pie
- [Models](./models) — Supported model families
- [CLI Reference](./cli) — Command-line interface documentation
- [Examples](./examples) — Annotated inferlet implementations
