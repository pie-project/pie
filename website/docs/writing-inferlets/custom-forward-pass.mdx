---
title: Custom Forward Pass
description: Control attention masks, position IDs, and logit masking
sidebar_position: 5
---

# Custom Forward Pass

The `ForwardPass` builder gives you direct control over a single forward pass through the model. While `ctx.generate()` abstracts this away for standard autoregressive generation, a custom forward pass enables advanced patterns like windowed attention, hierarchical attention, and custom causal masks.

## The ForwardPass Builder

```rust
use inferlet::inference::ForwardPass;

let pass = ForwardPass::new(&model);
pass.context(&ctx);
pass.input_tokens(&token_ids, &position_ids);
pass.sampler(&[sample_index], sampler);

let output = pass.execute_async().await?;
```

### Builder Methods

| Method | Description |
|--------|-------------|
| `.context(&ctx)` | Bind a context (KV cache) to this pass |
| `.input_tokens(&tokens, &positions)` | Set input token IDs with explicit position IDs |
| `.attention_mask(&brle_masks)` | Apply custom attention masks (list of BRLE per input token) |
| `.logit_mask(&brle_mask)` | Mask logits before sampling (BRLE-encoded) |
| `.sampler(&indices, sampler)` | Configure sampling at specific token positions |
| `.adapter(&adapter)` | Use a LoRA adapter for this pass |
| `.input_speculative_tokens(&tokens, &positions)` | Add draft tokens for verification |
| `.output_speculative_tokens(true)` | Request speculative tokens in output |
| `.execute_async()` | Run the pass asynchronously |

## BRLE Encoding

Pie uses **Bit Run-Length Encoding (BRLE)** for attention masks and logit masks. This compresses sparse binary masks into a sequence of run lengths:

```
 BRLE: [3, 2, 4, 1]

 Position: 0  1  2  3  4  5  6  7  8  9
 Mask:     0  0  0  1  1  0  0  0  0  1
           ╰3 zeros╯╰2 ones╯╰─4 zeros─╯╰1╯
           (attend)  (mask)  (attend)  (mask)
```

The encoding alternates between counts of zeros and ones, starting with zeros. This is efficient for masks that are mostly zeros (attend) with runs of ones (mask).

```
 Windowed attention mask example:
 Position: 0  1  2  3  4  5  6  7  8  9  10 11 12
           ◄─sink─►  ◄──evicted──►  ◄──window──►
 Mask:     0  0  0   1  1  1  1  1  0  0  0  0  0
 BRLE:     [3, 5, 5]
```

## Windowed Attention

A sliding window keeps only the most recent `N` tokens in the attention pattern. Tokens outside the window are masked, and their KV pages can be released:

```rust
use inferlet::inference::ForwardPass;
use inferlet::ContextExt;

let committed_len = ctx.committed_page_count() * ctx.tokens_per_page() + ctx.cursor();

if committed_len > window_size {
    let evict_count = committed_len - window_size;
    let pages_to_release = evict_count / ctx.tokens_per_page();

    if pages_to_release > 0 {
        // Release oldest pages from the KV cache
        ctx.release_pages(pages_to_release);
    }
}
```

When building the `ForwardPass`, apply an attention mask so the model ignores evicted positions:

```rust
// Build BRLE attention mask for each input token
// Each mask covers all past positions: [keep_sink, mask_evicted, keep_window]
let mask = vec![
    sink_size,                    // zeros (attend to sink tokens)
    evict_count - sink_size,      // ones (mask evicted tokens)
    window_size,                  // zeros (attend to window)
];

pass.attention_mask(&[mask]);  // One BRLE per input token
```

This is useful for tasks where only recent context matters, like streaming summarization or long-running dialogues.

## Attention Sink

An **attention sink** extends windowed attention by maintaining a fixed "sink" of initial tokens (which attention layers attend to heavily) plus a sliding window of recent tokens:

```
 Logical sequence:
 ┌─ sink ──┬───── evicted ──────┬──── window ────┐
 │ 0 1 2 3 │ 4 5 6 ... 95 96   │ 97 98 99 .. 127│
 └─────────┴────────────────────┴────────────────┘

 Physical KV cache (after release_pages):
 ┌─ sink ──┬──── window ────┐
 │ Page 0  │ Page 6  Page 7 │  ← only these pages remain
 └─────────┴────────────────┘

 Attention mask:  BRLE [4, 93, 31]
                  attend  mask  attend
```

The implementation evicts the middle portion and masks it in attention:

```rust
let max_cache_size = sink_size + window_size;

if committed_len > max_cache_size {
    let num_to_evict = committed_len - max_cache_size;
    let pages_to_release = num_to_evict / ctx.tokens_per_page();

    if pages_to_release > 0 {
        ctx.release_pages(pages_to_release);
    }
}
```

This is based on the [StreamingLLM](https://arxiv.org/abs/2309.17453) observation that initial tokens act as attention anchors.

## Custom Position IDs

The `input_tokens` method accepts explicit position IDs, enabling non-standard position assignments:

```rust
// Standard: sequential positions
let positions: Vec<u32> = (0..tokens.len() as u32).collect();

// Custom: skip positions, repeat, or reorder
let custom_positions = vec![0, 1, 2, 10, 11, 12]; // gap in middle
pass.input_tokens(&tokens, &custom_positions);
```

This is useful for:
- **Document hierarchies** — Assign position ranges to sections
- **Multi-document attention** — Separate position spaces per document
- **Positional interpolation** — Extend context length with scaled positions

## Logit Masking

Apply a logit mask to restrict which tokens can be sampled. This operates at the same level as the `Constrain` trait but can be set directly on a forward pass:

```rust
// BRLE-encoded mask over the vocabulary
// Zeros = allowed, Ones = masked
let mask = vec![100, 50, vocab_size - 150]; // only tokens 100..150 allowed
pass.logit_mask(&mask);
```

For grammar-based masking, see the [Structured Generation](./structured-generation) page which provides the higher-level `Constrain` trait.

## Next Steps

- **[Structured Generation](./structured-generation)** — Grammar-constrained output
- **[KV Cache Control](./kv-cache)** — Page-level cache management
- **[Speculative Decoding](./speculative-decoding)** — Draft and verify patterns
